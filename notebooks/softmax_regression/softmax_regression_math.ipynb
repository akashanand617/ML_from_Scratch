{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the fundamental mathematics behind softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will try to build an understanding based on the groundwork we laid for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "- Softmax will be using the concepts we had for logistic regression (such as logits and log likelihood) and generalizing it for $\\ge 2$ classes instead of the binary regression used before.\n",
    "- We will use an arbitrary class K as reference to map the rest of K-1 logits.\n",
    "- The summation will have to be mapped such that $\\sum^{K}_{i=1}P(y=k|i)=1$\n",
    "- Here, we will therefore generalize the sigmoid for more than 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we had $$log \\frac{P(y=1|x;\\theta)}{P(y=0|x;\\theta)}$$\n",
    "We will take reference class K and say that:\n",
    "$$log \\frac{P(y=1|x;\\theta)}{P(y=K|x;\\theta)} = \\theta_{10} + \\theta_1^TX \\\\\n",
    "log \\frac{P(y=2|x;\\theta)}{P(y=K|x;\\theta)} = \\theta_{20} + \\theta_2^TX \\\\\n",
    "\\vdots \\\\\n",
    "log \\frac{P(y=K-1|x;\\theta)}{P(y=K|x;\\theta)} = \\theta_{(K-1)0} + \\theta_{K-1}^TX\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k $\\in [1,K]$:\n",
    "$$\n",
    "\\theta_{k0}:\\text{kth bias} \\\\\n",
    "\\theta_k:\\text{kth vector of weights}\n",
    "$$\n",
    "Also\n",
    "$$\n",
    "P(y=k|x) = \\frac{e^{\\beta_{k0}+\\beta_k^TX}}{}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
